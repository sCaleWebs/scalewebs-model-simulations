---
title: "Beta diversity and species responses"
description: |
  A short description of the post.
author:
  - name: Andrew MacDonald
    url: https://example.com/norajones
date: 04-13-2022
output:
  distill::distill_article:
    self_contained: false
    toc: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
theme_set(theme_bw())
source(here::here("_posts/2022-04-13-beta-diversity-and-species-responses/simulation_functions.R"))
```

## Why simulate data?

The goal of this short document is to create simulated data which is resembles our sCaleWebs synthesis data.
This data will be simulated from a statistical model. 
This lets us set the values for various effects, and observe their effects on the food web patterns we are trying to understand.

In my view, a simulated data approach has several interesting advantages:

1. **understanding our models** ecological models can quickly become so complex, with many factors and emergent properities that it can be challenging to reason about them. Things like correlated predictors, sample size, and power can be investigated with 
1. **validate and test any statistical methods we develop** whatever approach we choose, it is helpful to study how it performs on simulated data. In our simulations, we know the _truth_ -- the values of specific parameters. A simple requirement for a model might be that it does not find an effect when none is there (no false positives) or that it usually finds an effect when one is present (limited false negatives)
1. **Parameter recovery**  For models which return parameter estimates, we can check to see if the model accurately recovers the _values_ of the parameters used to simulate the data. This can be very helpful, for example, in finding problems with identification.
1. **Understanding the data** Our dataset is complex, and our food webs are somewhat unique. For example, we make local webs by filtering the community metaweb with a bromeliad-level species list. The consequences of this are challenging to intuit; not least because this differs from many other datasets, where replicated food webs are directly observed.  Personally I find simulations helpful in understanding where a dataset comes from!


The simulations below have two parts: first, simulating plausible food webs; second, simulating responses to environmental gradients

### Food web simulations

I want to simulate the food web in a plausible way. Some constraints:

1. All species except basal resources need a link
2. most species will have only 3 connections (ie they eat basal resources; they are decomposers)

Here is an expression for the number of links.

<aside>
possibly with the constraint that the one link can't be a cannibal! 
</aside>

$$
L_i = (S - 1)p_i + 1
$$

The probability of a link, $p_i$ is another way of thinking about the degree of a species.

In our dataset the modal species richness is *3* because most species are detritivores that eat all the basal resources. 
One simple way to represent this is to allow the node distribution to have an average centered on 3 

<aside>
Do we want to have some kind of.. *3*-inflated distribution for our links? treat detritivores as a predictor category? or would taxonomy soak up a lot of this?
</aside>

The steps:
* find the average p for a food web where most species eat 3 things
* let each consumer have a different p 
* draw links for each predator

```{r links}
n_basal <- 3
S <- 12

mean_p <- (n_basal - 1)/(S - 1)

consumer_probs <- rbeta(n = S - n_basal, mean_p * 3, (1 - mean_p) * 3)

consumer_links <- rbinom(S - n_basal, S - 1, prob = consumer_probs) + 1

tibble(consumer_links) |> 
  count(consumer_links) |> 
  ggplot(aes(x = consumer_links, y = n)) + 
  geom_bar(stat = "identity") + 
  coord_cartesian(xlim = c(0, S))
```

The above uses the beta distribution to generate differences among consumers. 
However it is more flexible to use the logit scale: 

```{r}
simulate_degrees

degrees <- simulate_degrees(n_basal = 3, S = 12, degree_sd_logit = 0.3)

tibble(degrees) |> 
  count(degrees) |> 
  ggplot(aes(x = degrees, y = n)) + 
  geom_bar(stat = "identity") + 
  coord_cartesian(xlim = c(0, S))
```


### simulate incidence functions

$$
\begin{align}
Y_i &\sim \text{Binomial}(p, 1) \\
\text{logit}(p) &= a(\phi - b) \\
\phi &= \bar{\phi} + \beta_{e}X_e \\
\begin{bmatrix} a \cr b\ \end{bmatrix} &\sim \text{MVN}(\begin{bmatrix} \bar{a} \cr \bar{b}\ \end{bmatrix}, \Sigma)
\end{align}
$$

```{r}
nsites <- 45
x <- runif(nsites, min = -3, max = 3)

bar_phi <- -1
beta_phi <- .3

phi <- bar_phi + beta_phi * x

plot(x, plogis(phi))
```

simulate correlated $a$ and $b$

```{r}
corr_mat <- matrix(c(1, .5, .5, 1), nrow = 2)
sds <- c(.4, 1)

sig <- diag(sds) %*% corr_mat %*% diag(sds)

consumer_a_b <- MASS::mvrnorm(S - n_basal, mu = c(2, -3), Sigma = sig)
```

match species and sites to the coefs:

```{r}
fake_data <- expand_grid(site = 1:nsites, sp = 1:(S - n_basal)) |> 
  mutate(
    x = x[site],
    logit_prob = consumer_a_b[sp, 1] * (phi[site] - consumer_a_b[sp, 2]))

fake_data |> 
  ggplot(aes(x = x, y = logit_prob, group = sp)) + 
  geom_line()



fake_data |> 
  ggplot(aes(x = x, y = plogis(logit_prob), group = sp)) + 
  geom_line()

```

Now, we need to sample species compositions from this curve

```{r}
fake_data_obs <- fake_data |> 
  mutate(prb = plogis(logit_prob),
         presabs = rbinom(nrow(fake_data), size = 1, prob = prb)) |> 
  mutate(sp = paste0("sp", sp),
         site = paste0("site", site)) 

fake_rich <- fake_data_obs |> 
  group_by(site, x) |> 
  summarize(richness = sum(presabs)) |> 
  ungroup()

fake_rich |> 
  ggplot(aes(x = x, y = richness)) + geom_point()
```

Finally, we can tweak the simulation to include correlations between the response to the environment and the number of nodes

Could then look at the relationship between connectance and the environment, for example


```{r}
sxs <- fake_data_obs |> 
  select(site, sp, presabs) |> 
  pivot_wider(names_from = sp, values_from = presabs,values_fill = 0) |> 
  column_to_rownames("site")
```

```{r}
site_lcbd <- sxs |> 
  adespatial::beta.div() |> 
  pluck("LCBD") |> 
  enframe(name = "site", value = "lcbd")

fake_rich |> 
  left_join(site_lcbd) |> 
  ggplot(aes(x = richness, y = lcbd)) + geom_point()
```

```{r}
fake_rich |> 
  left_join(site_lcbd) |> 
  ggplot(aes(x = x, y = lcbd)) +
  geom_point() + 
  stat_smooth(method = "gam")
```

